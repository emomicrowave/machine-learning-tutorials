{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze mit Python und TensorFlow\n",
    "\n",
    "Im Gegensatz zu vorigen Artikel über TensorFlow, handelt sich hier eher um eine Anleitung, wie man TensorFlow verwendet um ein neuronales Netz zu erstellen. Der Artikel geht davon aus, dass man über Python-Kentnisse verfügt und TensorFlow bereits installiert hat. [(Wie installiert man TensorFlow)](https://www.tensorflow.org/install/)\n",
    "\n",
    "Wir fangen an mit einem Vergleich zwischen echten und künstlichen neuronale Netze:\n",
    "\n",
    "## Echte Neuronale Netzwerke\n",
    "\n",
    "![nervenzelle](images/neuron.png)\n",
    "\n",
    "*Nervenzelle; Autor: [Quasar Jarosz](https://en.wikipedia.org/wiki/User:Quasar_Jarosz); Licenz: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)*\n",
    "\n",
    "Das ist ein Neuron. Im menschlichen Gehirn gibt es 86 Milliarden davon. Die wichtigsten Strukturen sind:\n",
    "- **Dendrite** - erhalten Signale von anderen Nervenzellen. Ein Neuron kann sehr viele Dendrite haben\n",
    "- **Zellkörper** - summiert die Signale um Ausgabe zu generieren\n",
    "- **Axon** - wenn die Summe einen Schwellwert erreicht, wird ein Signal über den Axon übertragen. Nervenzelle haben immer nur einen Axon\n",
    "- **Axonterminale (Synapse)?** - Die Verbindungspunkt zwischen Axon und Dendriten. Die Stärke der Verbindung entspricht der Stärke der Stärke des übertragten Signal (synaptische Gewichte)\n",
    "\n",
    "## Künstliche Neuronale Netz\n",
    "\n",
    "Perzeptronen und Sigmoidneurone sind die Hauptbestandteile eines neuronalen Netzes. \n",
    "\n",
    "![Perzeptrone](images/perceptron.png)\n",
    "\n",
    "*Bild von Perzeptronen. Ein mit, der andere ohne Bias*\n",
    "\n",
    "Ähnlich wie ein Neuron, haben Perzeptronen mehrere **Inputs** (*x*) mit entsprechende **Gewichtungen** (*w*). Die Inputs sind immer 0 oder 1. Jedes Input wird mit einer der Gewichtungen multipliziert und man addiert noch ein **Bias** (*b oder Theta*) dazu. Am Ende summiert man alle Ergebnisse und verwendet einen **Schwellwert** um zu entscheiden, ob 1 oder 0 ausgegeben wird.\n",
    "\n",
    "Perzeptronen sind oft zu primitiv für künstliche neuronale Netze, deswegen benutzt man Sigmoidneuronen. Die Unterschied ist, dass sie auch eine **Aktivierungsfunktion** haben. \n",
    "\n",
    "![Sigmoidfunktion](images/sigmoid_fn.png)\n",
    "\n",
    "\n",
    "Diese Funktion erlaubt, dass diese Sigmoidneuronen als Inputs Zahlen zwischen 0 und 1 bekommen. Das hilft so, dass wenn man kleine Anpassungen an den Gewichtungen und Biasen macht, dann gibt es auch kleine Unterschiede bei der Ausgabe.\n",
    "\n",
    "![neuronales Netz](images/neural_network.png)\n",
    "\n",
    "*Neuronales Netz von [Glosser.ca](https://commons.wikimedia.org/wiki/User_talk:Glosser.ca), lizensiert unter [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)*\n",
    "\n",
    "Ein neuronales Netz besteht aus mehrere Schichten und jede Schicht hat mehrere Neuronen, wobei in der Regel hat jedes Neuron aus einer Schicht eine Verbindung zu allen anderen Neuronen aus der nächste Schicht. Die erste Schicht ist die Inputschicht, wo man sein Datensatz eingibt. Es kann beliebig viele Zwischenschichten geben, aber die letzte ist die Outputschicht, mit 1 oder mehreren Neuronen (Die Anzahl hängt vom Problem, den man lösen möchte ab). \n",
    "\n",
    "## Funktionsweise von TensorFlow\n",
    "\n",
    "- Tensor, Constant Variable\n",
    "- DataFlow graph\n",
    "\n",
    "## Implementierung eines neuronalen Netzes\n",
    "\n",
    "Wir werden ein eifaches neuronales Netz mit TensorFlow implementieren. Zusätzlich brauchen wir aber einige Funktionen aus *scikit-learn*. Wir werden den [Iris-Datensatz](https://en.wikipedia.org/wiki/Iris_flower_data_set) benutzen und versuchen mit dem neuronalen Netz die Blumen richtig zu klassifizieren.\n",
    "\n",
    "Zum ersten wollen wir Tensorflow und einige Funktionen aus *scikit-learn* importieren. Dann laden wir den Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datensatz\n",
    "data = load_iris()\n",
    "features = data.data\n",
    "labels = data.target.reshape((-1, 1))\n",
    "\n",
    "# Klassen von Blumen zeigen\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Wie man hier sieht, sind die Blummenklassen mit 0, 1 oder 2 bezeichnet. In der Regel, wenn man neuronale Netze für Klassifizierungsprobleme benutzen will, hat man so viele Neuronen in der letzten Schicht wie Klassen. Wir erwarten für jede Stichprobe eine Ausgabe, die so Aussieht `[0.15, 0.70, 0.15]`. In diesem Beispiel ist die Klasse der Blume `1`, weil die zweite Zahl die größte ist.  \n",
    "\n",
    "Wir wollen aber die Vorhersagen des neuronalen Netzes mit den echten Klassen vergleichen, deswegen wandeln wir die numerische Darstellung der Klassen zur sogenannten *One-hot encoding*. So hat man eine 3-Array als Bezeichner für jede Klasse.\n",
    "\n",
    "Beispiel:\n",
    "```\n",
    "Klasse 0 -> [1, 0, 0]\n",
    "Klasse 1 -> [0, 1, 0]\n",
    "Klasse 2 -> [0, 0, 1]\n",
    "```\n",
    "\n",
    "Wir verwenden dafür die `OneHotEncoder` von *scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainings- und Testdaten\n",
    "\n",
    "Um das Modell gut validieren zu können, sollen wir Daten verwenden, mit denen nie trainiert worden ist. Deswegen benutzen wir die `train_test_split` Funktion, um 20 Prozent der Daten als Testdaten zu nehmen. Dann merken wir uns die Anzahl von Merkmale bzw. von Klassen. In diesem Fall ist `x_size = 4` und `y_size = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trainings- und Testdaten erzeugen\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                features, \n",
    "                                enc.transform(labels) )\n",
    "\n",
    "x_size = train_x.shape[1]\n",
    "y_size = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholder** (Platzhalter) sind Tensors, die für die Dateneingabe sorgen. Man muss nur die Dimensionen spezifizieren. Zum Beispiel sind hier die Dimensionen für *X* `[None, x_size]`, weil wir eine unbestimmte Zahl von Stichproben haben mit jeweils `x_size` Merkmalen.\n",
    "\n",
    "In *X* geben wir die Merkmale ein, und in *Y* geben wir die korrekte Klassen ein, damit wir das Modell trainieren können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, x_size])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, y_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Für das Modell des neuronales Netzes benutzen wir 2 Zwischenschichten, mit jeweils 128 Neuronen. Wir benutzen verschiedene Aktivierungsfunktionen für die Zwischen- und Ausgabeschichten. Für die Ausgabeschicht benutzen wir die `softmax` Aktivierungsfunktion. Diese erlaubt uns eine Wahrscheinlichkeit für die Ausgabeklassen zu definieren, da die Summe aller Elemente aus der Ausgabeliste wegen der Aktivierungsfunktion gleich `1` ist. Beispiel für Ausgabe: `[0.10, 0.78. 0.12]`. Das heißt, dass das Modell 78% sicher ist, dass die aktuelle Stichprobe der Klasse `1` ist.\n",
    "\n",
    "Die `softmax` Funktion funktioniert nicht so gut für die Zwischenschichten, deswegen haben wir die `sigmoid` Aktivierungsfunktion ausgewählt.\n",
    "\n",
    "TensorFlow bietet viele Werkzeuge an, mit denen man ein Modell erstellen kann. Wir schauen uns hier zwei Möglichkeiten. Zum ersten definieren wir die Schichten mit TensorFlow Core. Man braucht mehr Codezeilen, dafür aber hat man mehr Kontrolle über das Program. Die zweite Lösung verwendet die API `tf.layers`, womit man schnell neue Schichten definieren kann.\n",
    "\n",
    "#### TensorFlow Core\n",
    "Um schneller das Modell erstellen zu können, definieren wir eine eigene Funktion `hidden_layer`, mit den folgenden Parametern.\n",
    "- `t_input` - Die Eingabetensor oder auch die vorige Schicht\n",
    "- `w_shape` - Eine 2-Array mit den Dimensionen der Schicht. Das erste Element ist die Neuronenanzahl der vorigen Schicht und das zweite - die Neuronenanzahl dieser Schicht.\n",
    "- `activation` - Aktivierungsfunktion. Da wir für verschiedene Schichten, unterschiedliche Aktivierungsfunktionen verwenden wollen, brauchen wir diese als Parameter einzugeben.\n",
    "\n",
    "Mit `random_normal(shape)` generieren wir zufällige Werte mit bestimmten Dimensionen für Gewichtungen und Biases. Danach bilden wir Unbekannten, deren Werte man mit einem Optimierer anpassen kann. \n",
    "\n",
    "Die algebraische Operationen, die im neuronalen Netz stattfinden, sind folgenderweise definiert:\n",
    "- man multipliziert das Input mit den Gewichtungen\n",
    "- man addiert die Biases dazu\n",
    "- man wendet die Aktivierungsfunktion an das Ergebnis an\n",
    "\n",
    "Man bemerkt, dass die Dimensionen der Gewichtungen und der Biases sich unterschieden. In unserem Fall hat das Input Dimensionen `1x4`, weil jede Stichprobe 4 Merkmale hat. Die Gewichtungen haben die Dimension `4x128`, weil wir 128 Neuronen haben und in jedem sollen wir eine Gewichtung pro Merkmal haben. Nachdem wir `tf.matmul()` ausführen, kommt ein Ergebnis heraus mit den Dimensionen `1x128`. Um die Biases dazu zu addieren, brauchen die dieselbe Dimensionen zu haben. \n",
    "\n",
    "\n",
    "#### `tf.layers`\n",
    "\n",
    "Mit `tf.layers` ist es sehr einfach ein neuronales Netz zu definieren. `tf.layers.dense` bietet uns das typische Schichtenmodell und behandelt automatisch die Addition von Biases und die Berechnung von Dimensionen. Wir geben nur das Eingabetensor, die Neuronenanzahl und die Aktivierungsfunktion ein. \n",
    "\n",
    "Es gibt auch andere Parameter, die man anpassen kann, die aber für uns im Moment nicht relevant sind. Bei Interesse, kann man sich die Dokumentation anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN Modell mit TF Core\n",
    "def hidden_layer(t_input, w_shape, activation=tf.nn.sigmoid):\n",
    "    weights = tf.Variable(tf.random_normal(w_shape))\n",
    "    biases = tf.Variable(tf.random_normal([1, w_shape[1]]))\n",
    "    \n",
    "    return activation(tf.add(tf.matmul(t_input, weights), biases))\n",
    "\n",
    "h_layer1 = hidden_layer(X, [x_size, 128])\n",
    "h_layer2 = hidden_layer(h_layer1, [128, 128])\n",
    "y_hat = hidden_layer(h_layer2, [128, y_size], tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MM Modell mit tf.layers\n",
    "h_layer1 = tf.layers.dense(X, 128, activation=tf.nn.sigmoid)\n",
    "h_layer2 = tf.layers.dense(h_layer1, 128, activation=tf.nn.sigmoid)\n",
    "y_hat = tf.layers.dense(h_layer2, y_size, activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostenfunktion, Optimierer und Session\n",
    "\n",
    "Wir brauchen jetzt eine **Kostenfunktion** um den Fehler zwischen den Vorhersgen und die echten Klassen zu berechnen. Die Tensorflow Funktion `softmax_cross_entropy_with_logits()` ist eine sehr gute Kostenfunktion, wenn man ein Klassifikationsproblem hat. Die Rückgabewert dieser Funktion ist eine Liste, mit den Fehlern der Vorhersage. Wir verwenden dann die Funktion `reduce_mean()` um den Mittelwert dieser Fehler zu berechnen, um nur mit einer Zahl zu arbeiten.\n",
    "\n",
    "Danach brauchen wir einen Optimierer, damit wir die Gewichtungen in den vorigen Schichten anpassen zu können. Den `GradientDescentOptimizer` kann man hier gut anwenden und das Ziel ist die Kostenfunktion zu minimieren. Als argument gibt man die Lernrate ein, wobei man darauf Achten muss, dass eine zu große Zahl zu einem ungenaueren Modell führen kann, und eine zu kleine Zahl zu lange Trainingszeiten führen kann.\n",
    "\n",
    "Bevor wir mit der Ausführung des Programms startet, müssen wir die Unbekannten initializieren. Diese sind die Gewichtungen und Biases in den Neuronenschichten, die der Optimierer anpasst, um die Kostenfunktion zu minimieren. Die Unbekannten haben am Anfang keine Werte und wir müssen den `global_variables_initializer()` ausführen um zufällige Werte da zu setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kosten- und Optimierungsfunktion\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=Y, logits=y_hat))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.005).minimize(loss)\n",
    "\n",
    "# Unbekannten initialisieren und Session erstellen\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsschleife\n",
    "\n",
    "Die erste Schleife ist für die Anzahl der Epochen. In diesem Fall wollen wir 300 mal alle Stichproben dem neuronalen Netz eingeben und zwar einer nach dem anderen (die zweite Schleife).\n",
    "\n",
    "Zum ersten führen wir `session.run()` mit `train_step` aus, also man berechnet die Kostenfunktion und den Optimierer. Deswegen geben wir die Trainingsdaten ein und alle 10 Epochen wollen wir die Genauigkeit evaluiren.\n",
    "\n",
    "Dafür verwenden wir `tf.argmax` um die Stelle der größten Zahl bei der Vorhersage und bei den echten Klassen zu vergleichen. Das gibt uns eine Liste von booleschen Werten (`True` oder `False`). Mithilfe von `cast(tf.float32)`, wandeln wir die boolesche Werte in `1` oder `0` um. Schließlich berechnen wir den Mittelwert dieser Liste. So bekommen wir eine Genauigkeit als Prozent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuraccy = 0.263158, Loss = 1.098506\n",
      "Epoch 10: Accuraccy = 0.552632, Loss = 1.074560\n",
      "Epoch 20: Accuraccy = 0.552632, Loss = 1.044309\n",
      "Epoch 30: Accuraccy = 0.552632, Loss = 0.995192\n",
      "Epoch 40: Accuraccy = 0.552632, Loss = 0.933063\n",
      "Epoch 50: Accuraccy = 0.552632, Loss = 0.885879\n",
      "Epoch 60: Accuraccy = 0.552632, Loss = 0.857331\n",
      "Epoch 70: Accuraccy = 0.552632, Loss = 0.829077\n",
      "Epoch 80: Accuraccy = 0.578947, Loss = 0.803954\n",
      "Epoch 90: Accuraccy = 0.736842, Loss = 0.781753\n",
      "Epoch 100: Accuraccy = 0.789474, Loss = 0.761336\n",
      "Epoch 110: Accuraccy = 0.894737, Loss = 0.741924\n",
      "Epoch 120: Accuraccy = 0.894737, Loss = 0.723349\n",
      "Epoch 130: Accuraccy = 0.921053, Loss = 0.705859\n",
      "Epoch 140: Accuraccy = 0.921053, Loss = 0.689829\n",
      "Epoch 150: Accuraccy = 0.921053, Loss = 0.675535\n",
      "Epoch 160: Accuraccy = 0.947368, Loss = 0.663070\n",
      "Epoch 170: Accuraccy = 0.947368, Loss = 0.652366\n",
      "Epoch 180: Accuraccy = 0.947368, Loss = 0.643252\n",
      "Epoch 190: Accuraccy = 0.973684, Loss = 0.635516\n",
      "Epoch 200: Accuraccy = 0.973684, Loss = 0.628942\n",
      "Epoch 210: Accuraccy = 0.973684, Loss = 0.623338\n",
      "Epoch 220: Accuraccy = 1.000000, Loss = 0.618536\n",
      "Epoch 230: Accuraccy = 1.000000, Loss = 0.614397\n",
      "Epoch 240: Accuraccy = 1.000000, Loss = 0.610807\n",
      "Epoch 250: Accuraccy = 1.000000, Loss = 0.607675\n",
      "Epoch 260: Accuraccy = 1.000000, Loss = 0.604924\n",
      "Epoch 270: Accuraccy = 1.000000, Loss = 0.602495\n",
      "Epoch 280: Accuraccy = 1.000000, Loss = 0.600339\n",
      "Epoch 290: Accuraccy = 0.973684, Loss = 0.598413\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(300):\n",
    "    for i in range(train_x.shape[0]):\n",
    "        sess.run(train_step, feed_dict={X: train_x[i:i+1], Y: train_y[i: i+1]})                                                 \n",
    "        \n",
    "    if (epoch % 10 == 0):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(Y, 1))                                                      \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                                                       \n",
    "        print(\"Epoch %d: Accuraccy = %f, Loss = %f\" % (                                                                          \n",
    "                          epoch,                                                                                                   \n",
    "                          sess.run(accuracy, feed_dict={X: test_x, Y: test_y}),                                                    \n",
    "                          sess.run(loss, feed_dict={X: train_x, Y: train_y})))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Man sieht, dass die Genauigkeit steigt und die Kostenfunktion sinkt. Man kann auch die Lernrate und die Anzahl von Neuronen in einer Schicht anpassen um die Veränderungen in der Genaigkeit sich anzuschauen.\n",
    "\n",
    "### Was kann man verbessern?\n",
    "\n",
    "- Man kann immer den Anzahl von Schichten und Neuronen anpassen\n",
    "- die Lernrate kann auch angepasst werden\n",
    "- um bessere Modelle zu bekommen, soll man die Daten bei jeder Epoche schlurfen\n",
    "- für bessere Laufzeit, soll man die Stichproben nicht eine nach den anderen dem neuronalen Netzes eingeben, sondern die in Stapel zusammenfassen. Die größe der Stapel hängt von der Anzahl von Merkmalen und dem vorhandenen Haupt- oder Videospeicherplatz ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ressoursen\n",
    "\n",
    "- Git Repository mit dem kompletten Code - [Link]()\n",
    "- Wie wählt man eine optimale Anzahl von Zwischenschichten und deren Nuronen? (Enghlisch) - [Link](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
