{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze mit Python und TensorFlow\n",
    "\n",
    "Im Gegensatz zum vorherigen Artikel über TensorFlow, handelt sich hier eher um ein Tutorial, wie man TensorFlow verwendet um ein neuronales Netz zu erstellen und zu trainieren. Der Artikel geht davon aus, dass man über Python-Kentnisse verfügt und TensorFlow und Scikit-Learn bereits installiert hat. [(Wie installiert man TensorFlow)](https://www.tensorflow.org/install/)\n",
    "\n",
    "Wir fangen an mit einem Vergleich zwischen echten und künstlichen neuronalen Netzen:\n",
    "\n",
    "## Echte Neuronale Netzwerke\n",
    "\n",
    "![nervenzelle](images/neuron.png)\n",
    "\n",
    "*Nervenzelle; Autor: [Quasar Jarosz](https://en.wikipedia.org/wiki/User:Quasar_Jarosz); Licenz: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)*\n",
    "\n",
    "Gezeigt ist ein Neuron. Im menschlichen Gehirn gibt es etwa 86 Milliarden davon. Die wichtigsten Strukturen sind:\n",
    "\n",
    "- **Dendrite** - erhalten Signale von anderen Nervenzellen, wobei ein Neuron kann sehr viele Dendrite haben\n",
    "- **Zellkörper** - summiert die Signale um die Ausgabe zu generieren\n",
    "- **Axon** - wenn die Summe einen Schwellwert erreicht, wird ein Signal über den Axon übertragen. Nervenzelle haben immer nur einen Axon\n",
    "- **Axonterminale (Synapse)** - Der Verbindungspunkt zwischen Axon und Dendriten. Die Stärke der Verbindung entspricht der Stärke des übertragten Signal (synaptische Gewichte)\n",
    "\n",
    "## Künstliche Neuronale Netz\n",
    "\n",
    "Perzeptronen und Sigmoidneurone sind die Hauptbestandteile eines neuronalen Netzes.\n",
    "\n",
    "![Perzeptrone](images/perceptron.png)\n",
    "\n",
    "*Bild von Perzeptronen. Ein mit, der andere ohne Bias*\n",
    "\n",
    "Ähnlich wie Neuronen, haben Perzeptronen mehrere **Inputs** (*x*) mit entsprechenden **Gewichtungen** (*w*). Die Outputs sind entweder 0 oder 1. Jeder Input wird mit einer entsprechenden Gewichtung multipliziert. Am Ende summiert man alle Ergebnisse und addiert noch die **Bias** (*b oder Theta*), welche dem **Schwellwert** entspricht. Angenommen, man hat *N* Inputs, sieht die Formel folgendermaßen aus:\n",
    "\n",
    "![Perceptron Equation](images/perceptron_equation.png)\n",
    "\n",
    "Ist die Gesamtsumme größer 0 ist der Output 1, ist die Summe kleiner 0 ist der Output entsprechend 0.\n",
    "\n",
    "Perzeptronen sind jedoch oft zu primiv. Nehmen wir an, wir wollen eine Gewichtung um einen kleinen Betrag ändern, so dass wir die Ausgabe des Netzweks entsprechend ändern. Bei Perzeptronen kann eine kleine Änderung der Gewichtungen dazu führen, dass der Output eines Neurons z.B plötzlich von 0 auf 1 umschlägt. Dies könnte das Verhalten des gesamten Netzwerks in einer unerwarteten Weise verändern. Um damit umzugehen, verwenden wir Sigmoidneuronen in neuronalen Netzen. Diese Art von Neuronen haben auch eine **Aktivierungsfunktion**, die als Sigmoidfunktion bezeichnet wird.  \n",
    "\n",
    "![Sigmoidfunktion](images/sigmoid_fn.png)\n",
    "\n",
    "Diese Funktion erlaubt, dass der Output der Sigmoidneuronen auch Zahlen zwischen 0 und 1 beinhaltet, statt nur 0 oder 1 wie im Perzeptron. Der große Vorteil ist, dass somit kleine Anpassungen an den Gewichtungen und Biasen nur in kleinen Unterschieden in der Ausgabe resultieren.\n",
    "\n",
    "![neuronales Netz](images/neural_network.png)\n",
    "\n",
    "*Neuronales Netz von [Glosser.ca](https://commons.wikimedia.org/wiki/User_talk:Glosser.ca), lizensiert unter [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)*\n",
    "\n",
    "Ein neuronales Netz besteht aus mehrere Schichten, wobei jede Schicht aus mehreren Neuronen besteht. In der Regel hat jedes Neuron aus einer Schicht Verbindungen zu allen anderen Neuronen aus der nächsten Schicht. Die erste Schicht ist die Inputschicht, wo der Datensatz eingeht. Dahinter können beliebig viele versteckte Schichten liegen, aber die letzte ist die Outputschicht, mit 1 oder mehreren Neuronen (Die Anzahl hängt vom Problem ab, dass man lösen möchte ab).\n",
    "\n",
    "## Funktionsweise von TensorFlow\n",
    "\n",
    "Die Hauptdatenstruktur in TensorFlow ist ein Tensor. Hierbei handelt es sich um ein Array von primitiven Datentypen, die in eine beliebige Anzahl von Dimensionen geformt sind. Der Rang eines Tensoren ist die Anzahl seiner Dimensionen.\n",
    "\n",
    "```python\n",
    "# Tensor Beispiele\n",
    "\n",
    "3 # Rang 0\n",
    "[1, 2, 3] # Rang 1\n",
    "[[1,2,3], [1,2,3]] # Rang 2\n",
    "[[[1,2,3], [1,2,3]]] # Rang 3\n",
    "```\n",
    "\n",
    "Einfache Arten von Tensoren sind Konstanten und Unbekannten (*Constant, Variable*). Konstanten sind, wie der Name schon sagt, Tensoren, deren Wert nicht verändert werden kann. Während der Ausführung des gesamten Programms behalten sie den gleichen Wert. Im Gegensatz können die Werte von Variablen geändert werden, weshalb sie als Gewichte und Bias für die verschiedenen Schichten der Modelle verwendet werden.\n",
    "\n",
    "Es ist auch wichtig, die Tensoren zu erwähnen, die das Ergebnis einer Operation mit anderen Tensoren enthalten. Beispiele sind Addition, Multiplikation oder die Anwendung einer komplexeren Funktion.\n",
    "\n",
    "![dataflow graph](images/tensors_flowing.gif)\n",
    "\n",
    "All dies bildet eine Art Graph, die aus allen Inputs, Outputs und Operationen dazwischen besteht. Die Knoten im Graphen können 0 oder mehr Tensoren aufnehmen und haben einen Tensor als Output. Um die gleiche Analogie wie die offizielle Seite zu verwenden, kann man einen Graphen als eine Quellcode-Detei beschreiben. Diese enthält Definitioen, die Datentypen von den Ein- und Ausgaben jeses Tensors, und die Reihenfolge der Operationen. Die Objekte im Graphen können jedoch nicht ausgewertet werden und müssen zuerst \"kompiliert\" werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elmilanov/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(2.0, name=\"const_2\")\n",
    "b = tf.constant(3.0, name=\"const_3\")\n",
    "c = a + b\n",
    "\n",
    "# Das zeigt uns nur, dass dieser Tensor die Operation Addition asuführt\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist zu sehen, dass `print(c)` nur eine Beschreibung des Tensors liefert. Um sie tatsächlich auszuwerten, benötigen wir ein sogenanntes **Session-Objekt**. Das Session-Objekt ist wie eine ausführbare Datei. Es kapselt die Umgebug, in der die Graph-Objekte ausgeführt und ausgewertet werden, durch Zuweisung von Ressourcen und ggf. Verteilung der Ausführung an verschiedene Geräte.\n",
    "\n",
    "Dieser Ansatz bietet die Möglichkeit, das Modell separat und parallel auf mehreren CPUs und GPUs laufen zu lassen, was zu schnellere Trainingszeiten führt. Außerdem hat man die Möglichkeit Teile des Graphen für andere Modelle wiederzuverwenden, oder den ganzen Graphen mittels **TensorBoard** zu visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mit einer Session können wir die Berechnung ausführen und das Ergebnis ausgeben\n",
    "sess = tf.Session()\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simple_graph](images/tf_simple_graph.png)\n",
    "\n",
    "*TensorBoard Visualisierung des Graphen von diesem Programm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementierung eines neuronalen Netzes\n",
    "\n",
    "Nun werden wir ein einfaches neuronales Netz mit TensorFlow implementieren. Zusätzlich brauchen wir aber einige Funktionen aus *scikit-learn*. Wir werden den [Iris-Datensatz](https://en.wikipedia.org/wiki/Iris_flower_data_set) benutzen und versuchen mit dem neuronalen Netz die Blumen richtig zu klassifizieren.\n",
    "\n",
    "Als erstes wollen wir Tensorflow und einige Funktionen aus *scikit-learn* importieren. Dann laden wir den Irisdatensatz. Wichtig ist `reshape((-1, 1))` auf den Blumenklassen auszuführen, da sie im Moment in einem eindimensionalen Array liegen, aber die `OneHotEncoder.fit_transform()` Funktion, die wir später benötigen, erfordert ein zweidimensionales Array als Eingabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datensatz\n",
    "data = load_iris()\n",
    "features = data.data\n",
    "labels = data.target.reshape((-1, 1))\n",
    "\n",
    "# Klassen von Blumen zeigen\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Wie man hier sieht, sind die Blummenklassen mit 0, 1 oder 2 bezeichnet. In der Regel, wenn man neuronale Netze für Klassifizierungsprobleme benutzen will, hat man so viele Neuronen in der letzten Schicht wie Klassen. Wir erwarten für jede Stichprobe eine Ausgabe, die so Aussieht `[0.15, 0.70, 0.15]`. In diesem Beispiel ist die Klasse der Blume 1, weil die zweite Zahl die größte ist.\n",
    "\n",
    "Wir wollen aber die Vorhersagen des neuronalen Netzes mit den echten Klassen vergleichen, deswegen wandeln wir die numerische Darstellung der Klassen zur sogenannten *One-hot encoding*. So hat man eine 3-Array als Bezeichner für jede Klasse.\n",
    "\n",
    "Beispiel:\n",
    "```\n",
    "Klasse 0 -> [1, 0, 0]\n",
    "Klasse 1 -> [0, 1, 0]\n",
    "Klasse 2 -> [0, 0, 1]\n",
    "```\n",
    "\n",
    "Wir verwenden dafür die `OneHotEncoder` von *scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainings- und Testdaten\n",
    "\n",
    "Um das Modell gut validieren zu können, sollen wir Daten verwenden, mit denen nie trainiert worden ist. Deswegen benutzen wir die `train_test_split` Funktion, um 20 Prozent der Daten als Testdaten zu nehmen. Dann merken wir uns die Anzahl von Merkmale bzw. von Klassen. In diesem Fall ist `x_size = 4` und `y_size = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trainings- und Testdaten erzeugen\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                features, \n",
    "                                enc.transform(labels) )\n",
    "\n",
    "x_size = train_x.shape[1]\n",
    "y_size = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholder** (Platzhalter) sind Tensors, die für die Dateneingabe sorgen. Man muss nur die Dimensionen spezifizieren. Zum Beispiel sind hier die Dimensionen für *X* `[None, x_size]`, weil wir eine unbestimmte Zahl von Stichproben haben mit jeweils `x_size` Merkmalen.\n",
    "\n",
    "In *X* geben wir die Merkmale ein, und in *Y* geben wir die korrekte Klassen ein, damit wir das Modell trainieren können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, x_size])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, y_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Modell des neuronales Netzes benutzen wir 2 versteckte Schichten, mit jeweils 128 Neuronen. Für die versteckten Schichten haben wir die `sigmoid`-Akitivierungsfunktion ausgewählt. Eine Akitvierungfunktion für die Ausgabenschicht wird hier jedoch nicht definiert, da die Konstenfunktion diese später übernimmt.\n",
    "\n",
    "Die `softmax` Funktion funktioniert nicht so gut für die Zwischenschichten, deswegen haben wir die `sigmoid` Aktivierungsfunktion ausgewählt.\n",
    "\n",
    "TensorFlow bietet viele Werkzeuge an, mit denen man ein Modell erstellen kann. Wir schauen uns hier zwei Möglichkeiten an. Zum ersten definieren wir die Schichten mit TensorFlow Core. Man braucht mehr Codezeilen, dafür aber hat man mehr Kontrolle über das Program. Die zweite Lösung verwendet die API `tf.layers`, womit man schnell neue Schichten definieren kann.\n",
    "\n",
    "#### TensorFlow Core\n",
    "Um schneller das Modell erstellen zu können, definieren wir eine eigene Funktion `hidden_layer`, mit den folgenden Parametern.\n",
    "- `input` - Die Eingabetensor oder auch die vorige Schicht\n",
    "- `shape` - Eine 2-Array mit den Dimensionen der Schicht. Das erste Element ist die Neuronenanzahl der vorigen Schicht und das zweite - die Neuronenanzahl der aktuellen Schicht.\n",
    "- `activation` - Aktivierungsfunktion. Da wir für verschiedene Schichten, unterschiedliche Aktivierungsfunktionen verwenden wollen, brauchen wir diese als Parameter einzugeben.\n",
    "\n",
    "Mit `random_normal(shape)` generieren wir zufällige Werte mit bestimmten Dimensionen für Gewichtungen und Biases. Danach bilden wir Unbekannten, deren Werte man mit einem Optimierer anpassen kann. \n",
    "\n",
    "Die algebraische Operationen, die im neuronalen Netz stattfinden, sind folgenderweise definiert:\n",
    "- man multipliziert das Input mit den Gewichtungen\n",
    "- man addiert die Biases dazu\n",
    "- man wendet die Aktivierungsfunktion an das Ergebnis an\n",
    "\n",
    "Man bemerkt, dass die Dimensionen der Gewichtungen und der Biases in den versteckten Schichten sich unterschieden. In der ersten Schicht hat das Input Dimensionen `1x4`, weil jede Stichprobe 4 Merkmale hat. Die Gewichtungen haben die Dimension `4x128`, weil wir 128 Neuronen haben und in jedem sollen wir eine Gewichtung pro Merkmal haben. Nachdem wir `tf.matmul()` ausführen, kommt ein Ergebnis heraus mit den Dimensionen 1x128. Um die Biases dazu zu addieren, brauchen die dieselbe Dimensionen zu haben. Bei einer Stichprobe **x**, Gewichtungen für jedes Neuron **W**, und Bias **b**, sieht die Formel für die erste versteckte Schicht folgendermaßen aus:\n",
    "\n",
    "![neural network equation](images/nn_equation.png)\n",
    "\n",
    "#### `tf.layers`\n",
    "\n",
    "Mit `tf.layers` ist es sehr einfach ein neuronales Netz zu definieren. `tf.layers.dense` bietet uns das typische Schichtenmodell (*Fully Connected Layer*) und behandelt automatisch die Addition von Biases und die Berechnung von Dimensionen. Wir geben nur das Eingabetensor, die Neuronenanzahl und die Aktivierungsfunktion ein. \n",
    "\n",
    "Es gibt auch andere Parameter, die man anpassen kann, die aber für uns im Moment nicht relevant sind. Bei Interesse, kann man sich die Dokumentation anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN Modell mit TF Core\n",
    "def hidden_layer(inputs, shape, activation=None):\n",
    "    weights = tf.Variable(tf.random_normal(shape))\n",
    "    biases = tf.Variable(tf.random_normal([1, shape[1]]))\n",
    "    \n",
    "    if activation:\n",
    "        return activation(tf.add(tf.matmul(inputs, weights), biases))\n",
    "    else:\n",
    "        return tf.add(tf.matmul(inputs, weights), biases)\n",
    "\n",
    "core_layer1 = hidden_layer(X, [x_size, 128], tf.nn.sigmoid)\n",
    "core_layer2 = hidden_layer(core_layer1, [128, 128], tf.nn.sigmoid)\n",
    "core_y_hat = hidden_layer(core_layer1, [128, y_size], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MM Modell mit tf.layers\n",
    "# wir wollen die Variablen nicht überschreiben\n",
    "h_layer1 = tf.layers.dense(X, 128, activation=tf.nn.sigmoid)\n",
    "h_layer2 = tf.layers.dense(h_layer1, 128, activation=tf.nn.sigmoid)\n",
    "y_hat = tf.layers.dense(h_layer2, y_size, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostenfunktion, Optimierer und Session\n",
    "\n",
    "Wir brauchen jetzt eine **Kostenfunktion** um den Fehler zwischen den Vorhersgen und die echten Klassen zu berechnen. Die Tensorflow Funktion `softmax_cross_entropy_with_logits_v2()` ist dem quadratischen Fehler ähnlich, mit dem Unterschied, dass sie die Eingaben als eine Wahrscheinlichkeitsverteilung interpretiert. Zuerst wird eine `softmax` Funktion auf den Eingaben ausgeführ. Diese erlaub uns eine Wahrscheinlichkeit für die Ausgabenklassen zu definieren, da die Summe aller Elemente aus der Ausgabeliste wegen der Aktivierungsfunktion gleich 1 ist. BEispiel für Ausgabe: `[0.10, 0,78, 0,12]`. Das heißt, dass das Modell zu 78% sicher ist, dass die aktuelle Stichprobe der Klasse 1 entspricht. Die Vorhersage wird mit der realen Klasse verglichen und der Fehler berehnet. Da die Eingaben der Kostenfunktion als eine Wahrscheilichkeitsverteilung interpretert werden, ist die Funktion eher für Klassifikationsprobleme geeignet.\n",
    "\n",
    "Die Rückgabewert dieser Funktion ist eine Liste mit den Fehlern der Vorhersage. Wir verwenden dann die Funktion `reduce_mean()`, um den Mittelwert dieser Fehler zu berechnen, so dass wir mit nur einer Zahl arbeiten können.\n",
    "\n",
    "Danach brauchen wir einen Optimierer, damit wir die Gewichtungen in den vorigen Schichten anpassen können. Der `GradientDescentOptimizer` kann hier gut eingesetzt werden und hat zum Ziel, die Kosten zu minimieren. Als argument gibt man die Lernrate ein. Diese gibt an, wie stark sich die Gewichtungen bei jeder Iteration des Optimierers ändern können. Man sollte darauf achten, dass eine kleine Lernrate das Training zwar zuverlässiger machen kann, aber die Optimierung wir länger dauern. Eine hohe Lernrate hingegen könnte zu einem ungenaeun Modell führen. Durch die großen Änderungen der Gewichtungen kann der Optimierer die besseren WErte überschreiten und somit die Kosten verschlechtern.\n",
    "\n",
    "Bevor wir mit der Ausführung des Programms starten, müssen wir die Unbekannten initializieren. Diese sind die Gewichtungen und Biases in den Neuronenschichten, die der Optimierer anpasst, um die Kosten zu minimieren. Die Unbekannten haben am Anfang keine Werte, deshalb müssen wir den `global_variables_initializer()` ausführen, um ihnen zufällige Werte zuzuweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Diese Variable modifizieren um die zwei Modelle zu testen\n",
    "# (y_hat ODER core_y_hat)\n",
    "y_pred = y_hat\n",
    "\n",
    "# Kosten- und Optimierungsfunktion\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=Y, logits=y_pred))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.0005).minimize(loss)\n",
    "\n",
    "# Unbekannten initialisieren und Session erstellen\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsschleife\n",
    "\n",
    "Die erste Schleife ist für die Anzahl der Epochen. In diesem Fall wollen wir 300 mal alle Stichproben dem neuronalen Netz eingeben und zwar einer nach dem anderen (die zweite Schleife).\n",
    "\n",
    "Zum ersten führen wir `session.run()` mit `train_step` aus, also man berechnet die Kostenfunktion und den Optimierer. Deswegen geben wir die Trainingsdaten ein und alle 10 Epochen wollen wir die Genauigkeit evaluiren.\n",
    "\n",
    "Dafür verwenden wir `tf.argmax` um die Stelle der größten Zahl bei der Vorhersage und bei den echten Klassen zu vergleichen. Das gibt uns eine Liste von booleschen Werten (`True` oder `False`). Mithilfe von `cast(tf.float32)`, wandeln wir die boolesche Werte in `1` oder `0` um. Schließlich berechnen wir den Mittelwert dieser Liste. So bekommen wir eine Genauigkeit als Prozent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuraccy = 0.315789, Loss = 1.159480\n",
      "Epoch 10: Accuraccy = 0.263158, Loss = 1.083721\n",
      "Epoch 20: Accuraccy = 0.263158, Loss = 1.072558\n",
      "Epoch 30: Accuraccy = 0.289474, Loss = 1.061280\n",
      "Epoch 40: Accuraccy = 0.631579, Loss = 1.049726\n",
      "Epoch 50: Accuraccy = 0.684211, Loss = 1.037734\n",
      "Epoch 60: Accuraccy = 0.684211, Loss = 1.025150\n",
      "Epoch 70: Accuraccy = 0.684211, Loss = 1.011829\n",
      "Epoch 80: Accuraccy = 0.684211, Loss = 0.997632\n",
      "Epoch 90: Accuraccy = 0.684211, Loss = 0.982428\n",
      "Epoch 100: Accuraccy = 0.684211, Loss = 0.966103\n",
      "Epoch 110: Accuraccy = 0.684211, Loss = 0.948561\n",
      "Epoch 120: Accuraccy = 0.684211, Loss = 0.929738\n",
      "Epoch 130: Accuraccy = 0.684211, Loss = 0.909607\n",
      "Epoch 140: Accuraccy = 0.684211, Loss = 0.888190\n",
      "Epoch 150: Accuraccy = 0.684211, Loss = 0.865571\n",
      "Epoch 160: Accuraccy = 0.684211, Loss = 0.841893\n",
      "Epoch 170: Accuraccy = 0.684211, Loss = 0.817365\n",
      "Epoch 180: Accuraccy = 0.684211, Loss = 0.792254\n",
      "Epoch 190: Accuraccy = 0.736842, Loss = 0.766866\n",
      "Epoch 200: Accuraccy = 0.736842, Loss = 0.741530\n",
      "Epoch 210: Accuraccy = 0.763158, Loss = 0.716568\n",
      "Epoch 220: Accuraccy = 0.763158, Loss = 0.692276\n",
      "Epoch 230: Accuraccy = 0.763158, Loss = 0.668899\n",
      "Epoch 240: Accuraccy = 0.789474, Loss = 0.646622\n",
      "Epoch 250: Accuraccy = 0.789474, Loss = 0.625565\n",
      "Epoch 260: Accuraccy = 0.789474, Loss = 0.605784\n",
      "Epoch 270: Accuraccy = 0.789474, Loss = 0.587283\n",
      "Epoch 280: Accuraccy = 0.815789, Loss = 0.570026\n",
      "Epoch 290: Accuraccy = 0.842105, Loss = 0.553942\n"
     ]
    }
   ],
   "source": [
    "# Trainingschleife \n",
    "for epoch in range(300):\n",
    "    for i in range(train_x.shape[0]):\n",
    "        sess.run(train_step, feed_dict={X: train_x[i:i+1], Y: train_y[i: i+1]})                                                 \n",
    "        \n",
    "    if (epoch % 10 == 0):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(Y, 1))                                                      \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                                                       \n",
    "        print(\"Epoch %d: Accuraccy = %f, Loss = %f\" % (                                                                          \n",
    "                          epoch,                                                                                                   \n",
    "                          sess.run(accuracy, feed_dict={X: test_x, Y: test_y}),                                                    \n",
    "                          sess.run(loss, feed_dict={X: train_x, Y: train_y})))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Man sieht, dass die Genauigkeit steigt und die Kostenfunktion sinkt. Man kann auch die Lernrate und die Anzahl von Neuronen in einer Schicht anpassen um die Veränderungen in der Genaigkeit sich anzuschauen.\n",
    "\n",
    "### Was kann man verbessern?\n",
    "\n",
    "- Man kann immer den Anzahl von Schichten und Neuronen anpassen\n",
    "- Die Lernrate kann auch angepasst werden. Noch besser ist es, die Lernrate dynamisch einzustellen, inde man mit einem höheren Wert beginnt und ihn während des Trainings verringert.\n",
    "- Um bessere Modelle zu bekommen, soll man die Daten bei jeder Epoche schlurfen\n",
    "- Für bessere Laufzeit, soll man die Stichproben nicht eine nach den anderen dem neuronalen Netzes eingeben, sondern die in Stapel zusammenfassen. Die größe der Stapel hängt von der Anzahl von Merkmalen und dem vorhandenen Haupt- oder Videospeicherplatz ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ressoursen\n",
    "\n",
    "- Git Repository mit dem kompletten Code - [Link](https://github.com/emomicrowave/machine-learning-tutorials/tree/master/91_neural_networks)\n",
    "- Wie wählt man eine optimale Anzahl von Zwischenschichten und deren Nuronen? (Enghlisch) - [Link](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
